{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    boston_data=pd.DataFrame(load_boston().data,columns=load_boston().feature_names)\n",
    "    Y=load_boston().target\n",
    "    X=load_boston().data\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_stochastic_gradient(b, w, data_x, data_y, learning_rate):\n",
    "    y = np.array([data_y]).T\n",
    "    data = np.hstack((data_x,y))\n",
    "    data_len = data.shape[0]\n",
    "    w_len = w.shape[1]\n",
    "    \n",
    "    #random suffle\n",
    "    data = data.sample(1)\n",
    "    \n",
    "    y=np.array(data[:,-1])\n",
    "    x=np.delete(data, -1, axis=1)\n",
    "    \n",
    "    w_gradient=np.zeros(shape=(1,x.shape[1]))\n",
    "    b_gradient=0\n",
    "    \n",
    "    prediction=np.dot(w,x[0])+b\n",
    "    w_gradient=w_gradient+(-2)*x[0]*(y[0]-(prediction))\n",
    "    b_gradient=b_gradient+(-2)*(y[0]-(prediction))\n",
    "    \n",
    "    w=w-learning_rate*(w_gradient)\n",
    "    b=b-learning_rate*(b_gradient)\n",
    "    \n",
    "    return b,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_runner(data_x, data_y, starting_b, initial_w, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        b, w = step_stochastic_gradient(b, w, data_x, data_y, learning_rate)\n",
    "    return [b, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x,w,b):\n",
    "    y_pred=[]\n",
    "    for i in range(len(x)):\n",
    "        y=np.asscalar(np.dot(w,x[i])+b)\n",
    "        y_pred.append(y)\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    x_train,x_test,y_train,y_test = load_data()\n",
    "    initial_w=np.zeros(shape=(1,x_train.shape[1]))\n",
    "    initial_b = 0\n",
    "    learning_rate = 0.01\n",
    "    iteration = 100\n",
    "    b, w = stochastic_gradient_descent_runner(x_train, y_train, initial_b, initial_w, learning_rate, iteration)\n",
    "    y_pred_customsgd=predict(x_test,w,b)\n",
    "    print(y_pred_customsgd)\n",
    "    print('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.47094860e+175 -9.06355489e+174 -1.03758631e+175 -1.00618415e+175\n",
      " -1.00796081e+175 -1.00358834e+175 -9.85589180e+174 -8.64448478e+174\n",
      " -8.76453347e+174 -1.12100110e+175 -9.26742928e+174 -1.06660268e+175\n",
      " -1.20108161e+175 -1.04946399e+175 -1.01856468e+175 -9.98022902e+174\n",
      " -1.06502173e+175 -1.47340796e+175 -9.68747755e+174 -8.77603589e+174\n",
      " -9.45243782e+174 -9.92518995e+174 -1.19726518e+175 -1.50630359e+175\n",
      " -9.03159102e+174 -1.50276744e+175 -1.21592279e+175 -9.46992820e+174\n",
      " -1.47888729e+175 -8.78784550e+174 -1.49976350e+175 -1.19365137e+175\n",
      " -1.47925835e+175 -9.64708426e+174 -1.37562543e+175 -1.12856759e+175\n",
      " -9.60718314e+174 -1.01268522e+175 -8.56750348e+174 -9.66493251e+174\n",
      " -1.00546911e+175 -1.50426361e+175 -9.99614884e+174 -1.17225283e+175\n",
      " -9.94595463e+174 -1.01036388e+175 -9.56705578e+174 -1.02984640e+175\n",
      " -1.40091639e+175 -1.13623553e+175 -8.79768743e+174 -1.03075273e+175\n",
      " -6.29333165e+174 -1.04181232e+175 -9.75085463e+174 -8.97978900e+174\n",
      " -1.00782653e+175 -9.28267562e+174 -1.17082758e+175 -8.57473343e+174\n",
      " -9.35076797e+174 -1.08486937e+175 -1.10808773e+175 -1.50638509e+175\n",
      " -9.22765064e+174 -1.07049291e+175 -1.00000495e+175 -1.02638833e+175\n",
      " -1.13330145e+175 -9.48554327e+174 -9.53628714e+174 -9.88146599e+174\n",
      " -1.21382862e+175 -9.55361335e+174 -1.14029592e+175 -9.82786113e+174\n",
      " -1.30406396e+175 -1.49123092e+175 -1.04906584e+175 -1.50071027e+175\n",
      " -1.20670347e+175 -1.35953240e+175 -9.11530043e+174 -1.07553141e+175\n",
      " -1.03125139e+175 -9.56494832e+174 -1.50036183e+175 -1.14680961e+175\n",
      " -1.21520108e+175 -1.01971491e+175 -1.03759415e+175 -1.05733168e+175\n",
      " -1.27947825e+175 -9.47077631e+174 -1.12694606e+175 -1.47924246e+175\n",
      " -1.09423701e+175 -1.13783225e+175 -1.12284712e+175 -1.21151298e+175\n",
      " -9.88285105e+174 -1.03069127e+175 -9.05898006e+174 -9.33044835e+174\n",
      " -1.03315795e+175 -1.50022585e+175 -1.13155872e+175 -8.92843462e+174\n",
      " -6.79453517e+174 -9.89273648e+174 -9.71280442e+174 -1.00262982e+175\n",
      " -9.60312119e+174 -9.89046629e+174 -1.20035699e+175 -1.08704447e+175\n",
      " -1.01045934e+175 -9.21844232e+174 -1.13614968e+175 -9.43693246e+174\n",
      " -9.42252698e+174 -1.37592519e+175 -1.35701974e+175 -9.84744997e+174\n",
      " -1.43059413e+175 -8.83780916e+174 -9.31062022e+174 -1.00414092e+175\n",
      " -1.14271937e+175 -8.83144653e+174 -1.45056507e+175 -1.03824167e+175\n",
      " -9.05795728e+174 -1.00333739e+175 -1.11879521e+175 -1.14431571e+175\n",
      " -1.47276444e+175 -9.15475631e+174 -9.11202009e+174 -9.24011768e+174\n",
      " -1.50477851e+175 -9.26987613e+174 -1.13041577e+175 -1.00972025e+175\n",
      " -1.04240231e+175 -1.02422306e+175 -1.07799725e+175 -1.14333060e+175\n",
      " -1.05986597e+175 -9.36109159e+174 -1.49794066e+175 -1.04384768e+175]\n",
      "Mean Squared Error : inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/regression.py:243: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
