\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Topic Modeling},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Topic Modeling}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

How to select the hyperparameters in topic models is the most crucial
part of any topic modeling task.

For example, the \textbf{alpha} parameter controls the convergence of
document-topic distribution. A small alpha gives a strong weight to the
most influential topic for the document, meaning the model will more
likely to consider the document is composed of a small number of topics,
and vice versa. \emph{A rule of thumb given by Griffiths \&
Steyvers(2004) is to use 50/k, where k is the number of topics.}

Another example is \textbf{beta} (or delta in Gibbs Sampling code),
which controls the convergence of the word distribution under each
topic. Similar to alpha, when a small beta (delta) is given, the model
will most likely to choose a common word from the topic, leading to
several peaks in the topic-word distribution. As beta grows bigger,
dramatic peaks will start dissappearing, and the model will be more
``flat''. \emph{The rule of thumb is to set beta (delta) equal to 0.1.}

While the above mentioned parameteres have a general strategy for
selection, how to determine the \textbf{cut-off point for top words} and
\textbf{the number of topics} is not covered. There is not really one
best metric for these, and may depend on the task at hand. For example,
if the user wishes to get a big picture of the corpus by topic modeling,
the user should use a small number of topics to avoid information
overloading, but oftentimes autonomous methods lead to a higher number
of topics.

In this project, we are to explore two tasks:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How many top words should be picked from the topics:

  \begin{itemize}
  \tightlist
  \item
    Cut-off point
  \end{itemize}
\item
  How many topics should be used:

  \begin{itemize}
  \tightlist
  \item
    Perplexity
  \item
    Extrinsic Method
  \item
    Intrinsic Method
  \item
    Document clustering
  \end{itemize}
\end{enumerate}

\hypertarget{submission-instructions}{%
\subsection{Submission Instructions}\label{submission-instructions}}

Complete the missing components of this RMD file and then submit a zip
file that contains this RMD file and the generated PDF or html document
showing the code and its output.

\hypertarget{loading-relevant-packages}{%
\subsection{Loading Relevant Packages}\label{loading-relevant-packages}}

Here are some packages you will probably want to load. You may have to
install these packages if you haven't before. Also, if you use other
packages in the rest of your code, add the packages to this list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(topicmodels)}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{library}\NormalTok{(tm)}
\KeywordTok{library}\NormalTok{(stringr)}
\NormalTok{SEED =}\StringTok{ }\DecValTok{3456}
\KeywordTok{data}\NormalTok{(}\StringTok{"AssociatedPress"}\NormalTok{, }\DataTypeTok{package=}\StringTok{"topicmodels"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{selecting-top-words}{%
\section{Selecting Top Words}\label{selecting-top-words}}

To determine the number of top words to be selected, we can leverage the
posterior probabilities of p(Word\textbar{}Topic) and use the
\textbf{elbow method}. The elbow method is an inexact technique that can
be used in various applications to help determine a good value for some
parameter by looking at a plot, where the x-axis contains the parameter
value (in this case, number of words) and the y-axis contains the
variable of interest (in this case, probabilities), and determining an
x-value where there is an ``elbow'' or different rate of change that
creates an angle in the plot.

First, let's run LDA with 20 topics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LDA_model <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(AssociatedPress, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{iter=}\DecValTok{20}\NormalTok{, }\DataTypeTok{seed=}\NormalTok{SEED), }\DataTypeTok{k=}\DecValTok{20}\NormalTok{, }\DataTypeTok{method=}\StringTok{"Gibbs"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, find the top 100 probabilities, p(Word\textbar{}Topic), and their
corresponding words. To do this, for each word (column in the @beta
matrix of the LDA model), find the max p(Word\textbar{}Topic) value.
This will give you a list of the highest probabilities each word has for
all the topics. Sort this list to find the top-100.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{wordsProb <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{exp}\NormalTok{(LDA_model}\OperatorTok{@}\NormalTok{beta), }\DecValTok{2}\NormalTok{, max)}
\NormalTok{sortedByProbWords <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(wordsProb, }\DataTypeTok{decreasing=}\NormalTok{T, }\DataTypeTok{index.return=}\NormalTok{T)}
\NormalTok{top100Values <-}\StringTok{ }\NormalTok{sortedByProbWords}\OperatorTok{$}\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{]}
\NormalTok{top100Words <-}\StringTok{ }\NormalTok{LDA_model}\OperatorTok{@}\NormalTok{terms[sortedByProbWords}\OperatorTok{$}\NormalTok{ix[}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

Now, plot the top-100 values.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\KeywordTok{plot}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{, top100Values, }\StringTok{"o"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Word"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Max Probability"}\NormalTok{)}
\KeywordTok{points}\NormalTok{(}\DecValTok{10}\NormalTok{, top100Values[}\DecValTok{10}\NormalTok{], }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{topic_model_files/figure-latex/unnamed-chunk-4-1.pdf}

Based on your plot, what seems like a reasonable cut off point for the
number of top words? Print these words.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{top100Words[}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "percent" "million" "i"       "soviet"  "court"   "south"   "police" 
##  [8] "billion" "bush"    "company"
\end{verbatim}

\hypertarget{number-of-topics}{%
\section{Number of topics}\label{number-of-topics}}

In the previous section, we performed LDA with 20 topics; however, you
may not know how many topics there should be. Now, we will explore
methods for determining the number of topics to use.

\hypertarget{perplexity}{%
\subsection{Perplexity}\label{perplexity}}

Perplexity often requires splitting the dataset into a training and test
set to evaluate the model. The intuition behind perplexity is to compare
the word log-likelihood with the test data. If highly probable words in
the model are also common words in test data, then it means the topic
model summarizes the data well.

Build ten LDA models by spanning the number of topics k from 20 to 200
in increments of 20. Use the first 1500 documents of AssociatedPress as
the training set and the remaining documents as the test set (for the
calculation of perplexity). Plot the perplexity values for these ten LDA
models, which you can determine using the \texttt{perplexity} function
from the topicmodels package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train <-}\StringTok{ }\NormalTok{AssociatedPress[}\DecValTok{1}\OperatorTok{:}\DecValTok{1500}\NormalTok{,]}
\NormalTok{test <-}\StringTok{ }\NormalTok{AssociatedPress[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1500}\NormalTok{),]}
\NormalTok{LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{20}\NormalTok{))\{}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(train, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{iter=}\DecValTok{20}\NormalTok{, }\DataTypeTok{seed=}\NormalTok{SEED), }\DataTypeTok{k=}\NormalTok{k, }\DataTypeTok{method=}\StringTok{"Gibbs"}\NormalTok{)}
\NormalTok{  LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{(LDA_models, model)}
\NormalTok{\}}
\NormalTok{perplex <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(LDA_models, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{perplexity}\NormalTok{(x, test))}
\KeywordTok{plot}\NormalTok{(perplex, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{axes=}\NormalTok{F, }\DataTypeTok{ann=}\NormalTok{F)}
\KeywordTok{axis}\NormalTok{(}\DecValTok{1}\NormalTok{,  }\DataTypeTok{at=}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{lab=}\KeywordTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{20}\NormalTok{))}
\KeywordTok{axis}\NormalTok{(}\DecValTok{2}\NormalTok{,  }\DataTypeTok{at=}\KeywordTok{seq}\NormalTok{(}\DecValTok{2500}\NormalTok{,}\DecValTok{3500}\NormalTok{,}\DecValTok{100}\NormalTok{), }\DataTypeTok{lab=}\KeywordTok{seq}\NormalTok{(}\DecValTok{2500}\NormalTok{,}\DecValTok{3500}\NormalTok{,}\DecValTok{100}\NormalTok{))}
\KeywordTok{title}\NormalTok{(}\DataTypeTok{xlab=}\StringTok{"Number of topics"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Perplexity"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Perplexity growth"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{topic_model_files/figure-latex/unnamed-chunk-6-1.pdf}

Note that perplexity drops infinitely as number of topic grows. So
instead of finding the minimum perplexity, common practice selects the
elbow point of perplexity growth.

\hypertarget{extrinsic-topic-coherence}{%
\subsection{Extrinsic Topic Coherence}\label{extrinsic-topic-coherence}}

Extrinsic (in contrast to intrinsic) requires an additional dataset. The
extrinsic topic coherence measure takes the top N words in each topic
and sees how often does the word pair appears in a common corpus (e.g.,
Wikipedia).

Here we use the Normalized Pointwise Mutual Information (NPMI) metric
(Nguyen et al, 2015), though there are many other extinsic metrics
available that work similarly.

For simplicity and performance reasons (loading Wikipeida is very slow),
we use the Yelp dataset and split it into training (1k reviews) and a
common corpus (19K reviews), testing the model on the latter part. But
notice this is not a common approach, and, in practice, should be
performed on a larger corpus.

Also, in the original paper (Nguyen et al, 2015), the author uses a
sliding window approach to model word pair co-occurence. For simplicity,
we take co-occurence based on co-occurred words in documents, and
further penalized it to make NPMI measure negative. First, lets load the
Yelp dataset and clean it up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yelp =}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"yelp.txt"}\NormalTok{, }\DataTypeTok{header=}\OtherTok{FALSE}\NormalTok{, }\DataTypeTok{quote=}\StringTok{""}\NormalTok{, }\DataTypeTok{sep=}\StringTok{"|"}\NormalTok{)}
\NormalTok{yelp_text =}\StringTok{  }\KeywordTok{as.list}\NormalTok{(}\KeywordTok{levels}\NormalTok{(yelp}\OperatorTok{$}\NormalTok{V1))}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"&amp"}\NormalTok{, }\StringTok{""}\NormalTok{, yelp_text)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"(RT|via)((?:}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{b}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{W*@}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{w+)+)"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"@}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[[:punct:]]"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[[:digit:]]"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"http}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{w+"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[ }\CharTok{\textbackslash{}t}\StringTok{]\{2,\}"}\NormalTok{, }\StringTok{" "}\NormalTok{, clean_yelp)}
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"[ }\CharTok{\textbackslash{}n}\StringTok{]\{2,\}"}\NormalTok{, }\StringTok{" "}\NormalTok{, clean_yelp) }
\NormalTok{clean_yelp =}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"^}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+|}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{s+$"}\NormalTok{, }\StringTok{""}\NormalTok{, clean_yelp) }
\NormalTok{clean_yelp <-}\StringTok{ }\KeywordTok{str_replace_all}\NormalTok{(clean_yelp,}\StringTok{" "}\NormalTok{,}\StringTok{" "}\NormalTok{)}
\NormalTok{clean_yelp <-}\StringTok{ }\KeywordTok{iconv}\NormalTok{(clean_yelp, }\StringTok{'UTF-8'}\NormalTok{, }\StringTok{'ASCII'}\NormalTok{,}\DataTypeTok{sub =} \StringTok{""}\NormalTok{)}

\NormalTok{yelp_Corpus <-}\StringTok{ }\KeywordTok{Corpus}\NormalTok{(}\KeywordTok{VectorSource}\NormalTok{(clean_yelp))}
\NormalTok{yelp_matrix <-}\StringTok{ }\KeywordTok{DocumentTermMatrix}\NormalTok{(yelp_Corpus,}\DataTypeTok{control =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{tolower =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sparse=}\OtherTok{TRUE}\NormalTok{, }\DataTypeTok{stemming =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{stopwords =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{minWordLength =} \DecValTok{3}\NormalTok{,}\DataTypeTok{removeNumbers =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{removePunctuation =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{yelp_matrix <-}\StringTok{ }\KeywordTok{removeSparseTerms}\NormalTok{(yelp_matrix, }\FloatTok{0.995}\NormalTok{)}
\NormalTok{rowTotals <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(yelp_matrix , }\DecValTok{1}\NormalTok{, sum) }
\NormalTok{yelp_matrix   <-}\StringTok{ }\NormalTok{yelp_matrix[rowTotals}\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{, ]  }\CommentTok{#removing documents that became empty after processing  }
\NormalTok{yelp_matrix <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(yelp_matrix)}
\end{Highlighting}
\end{Shaded}

Next, we will create the NPMI and coherence function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NPMI =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(DT, m,l)\{  }
\NormalTok{  number_of_documents =}\StringTok{ }\KeywordTok{dim}\NormalTok{(DT)[}\DecValTok{1}\NormalTok{]}
\NormalTok{  p_ml =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,l] }\OperatorTok{>}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{DT[,m] }\OperatorTok{>}\DecValTok{0}\NormalTok{))  }\OperatorTok{/}\StringTok{ }\NormalTok{(number_of_documents }\OperatorTok{*}\StringTok{ }\NormalTok{number_of_documents)}
  
\NormalTok{  p_l =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,l] }\OperatorTok{>}\DecValTok{0}\NormalTok{))}\OperatorTok{/}\NormalTok{number_of_documents}
  
\NormalTok{  p_m =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,m] }\OperatorTok{>}\DecValTok{0}\NormalTok{))}\OperatorTok{/}\NormalTok{number_of_documents}
  \CommentTok{# p_ml: probability of word m and word l both appears in a document}
  \CommentTok{# p_l: probability of word l appears in a document}
  \CommentTok{# p_m: probability of word m appears in a document}
  \ControlFlowTok{if}\NormalTok{ (p_ml}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
    \KeywordTok{return}\NormalTok{(}\DecValTok{0}\NormalTok{)}
  \ControlFlowTok{else}
    \KeywordTok{return}\NormalTok{( }\KeywordTok{log}\NormalTok{( p_ml  }\OperatorTok{/}\StringTok{ }\NormalTok{(p_l }\OperatorTok{*}\StringTok{ }\NormalTok{p_m)) }\OperatorTok{/}\StringTok{ }\OperatorTok{-}\KeywordTok{log}\NormalTok{(p_ml) )}
  
\NormalTok{\}}

\NormalTok{compute_c <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(LDA_model, dataset,  top_N, }\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"LCP"}\NormalTok{, }\StringTok{"NPMI"}\NormalTok{), }\DataTypeTok{top_K=} \DecValTok{0}\NormalTok{)\{}
\NormalTok{  c =}\StringTok{ }\KeywordTok{list}\NormalTok{()}
  \ControlFlowTok{if}\NormalTok{(method }\OperatorTok{==}\StringTok{ "LCP"}\NormalTok{)}
\NormalTok{    method =}\StringTok{ }\NormalTok{LCP}
  \ControlFlowTok{else}
\NormalTok{    method =}\StringTok{ }\NormalTok{NPMI}
\NormalTok{  top_words <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{t}\NormalTok{(LDA_model), }\DecValTok{2}\NormalTok{, }\DataTypeTok{FUN =} \ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sort}\NormalTok{(x, }\DataTypeTok{decreasing =}\NormalTok{ T,}\DataTypeTok{index.return =}\NormalTok{ T)}\OperatorTok{$}\NormalTok{ix[}\DecValTok{1}\OperatorTok{:}\NormalTok{top_N]) }\CommentTok{#find top N words}

  \CommentTok{#the following nested for-loop computes NPMI or LCP for all word pairs in top N for all topics}
  \ControlFlowTok{for}\NormalTok{( i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(top_words)[}\DecValTok{2}\NormalTok{])\{}
\NormalTok{    temp_c =}\StringTok{ }\DecValTok{0}
    \ControlFlowTok{for}\NormalTok{( m }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\NormalTok{top_N)\{}
      \ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\StringTok{ }\NormalTok{(m}\DecValTok{-1}\NormalTok{))\{}
\NormalTok{          temp_c =}\StringTok{ }\NormalTok{temp_c }\OperatorTok{+}\StringTok{ }\KeywordTok{method}\NormalTok{(dataset,top_words[m,i],top_words[l,i])}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    c[[i]] =}\StringTok{ }\NormalTok{temp_c}
\NormalTok{  \}}
\NormalTok{  c =}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(c)}
  \ControlFlowTok{if}\NormalTok{(top_K }\OperatorTok{==}\StringTok{ }\DecValTok{0}\NormalTok{)}
    \KeywordTok{return}\NormalTok{( }\KeywordTok{sum}\NormalTok{(c)}\OperatorTok{/}\KeywordTok{dim}\NormalTok{(LDA_model)[}\DecValTok{1}\NormalTok{])}
  \ControlFlowTok{else}
    \KeywordTok{return}\NormalTok{( }\KeywordTok{sum}\NormalTok{(}\KeywordTok{sort}\NormalTok{(c, }\DataTypeTok{decreasing =}\NormalTok{ T,}\DataTypeTok{index.return =}\NormalTok{ T)}\OperatorTok{$}\NormalTok{x[}\DecValTok{1}\OperatorTok{:}\NormalTok{top_K]) }\OperatorTok{/}\StringTok{ }\NormalTok{top_K  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Build ten LDA models by spanning the number of topics k from 10 to 100
in increments of 10. Use the first 1000 documents of Yelp as the
training set and the remaining documents of Yelp as the test set (for
the calculation of coherence). Plot the NPMI coherence values
(\texttt{compute\_c} function) for these fifteen LDA models. For your
NPMI coherence calculation, you can use top\_N=15. Since we have a small
training set, increase the iterations for LDA to 100.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{train <-}\StringTok{ }\NormalTok{yelp_matrix[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{,]}
\NormalTok{test <-}\StringTok{ }\NormalTok{yelp_matrix[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{),]}
\NormalTok{LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{))\{}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(train, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{iter=}\DecValTok{100}\NormalTok{, }\DataTypeTok{seed=}\NormalTok{SEED), }\DataTypeTok{k=}\NormalTok{k, }\DataTypeTok{method=}\StringTok{"Gibbs"}\NormalTok{)}
\NormalTok{  LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{(LDA_models, model)}
\NormalTok{\}}
\NormalTok{top_n =}\StringTok{ }\DecValTok{15}
\NormalTok{NPMI_results =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(LDA_models, }\ControlFlowTok{function}\NormalTok{(LDA_model) }\KeywordTok{compute_c}\NormalTok{(LDA_model}\OperatorTok{@}\NormalTok{beta, test, top_n, }\StringTok{"NPMI"}\NormalTok{))}
\NormalTok{NPMI_results =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{), NPMI_results)}
\KeywordTok{plot}\NormalTok{(NPMI_results, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Number of topics"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"NPMI"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Extrinsic topic coherence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{topic_model_files/figure-latex/unnamed-chunk-9-1.pdf}

Similar to perplexity, selecting a cut-off point is better since it
grows infinitely.

\hypertarget{intrinsic-topic-coherence}{%
\subsection{Intrinsic Topic Coherence}\label{intrinsic-topic-coherence}}

Similar to extrinsic methods, the intrinsic topic coherence measure
takes the top N words in each topic, and sees how often does the word
pair appear in the training corpus. Similar to perplexity, selecting a
cut-off point is better since it grows infinitely.

First, let's define the log-likelihood metric for intrinsic coherence.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LCP =}\StringTok{ }\ControlFlowTok{function}\NormalTok{(DT, m,l )\{  }
\NormalTok{  D_ml =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,m] }\OperatorTok{>}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{DT[,l] }\OperatorTok{>}\DecValTok{0}\NormalTok{)) }
\NormalTok{  D_l =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,l] }\OperatorTok{>}\DecValTok{0}\NormalTok{))}
\NormalTok{  D_m =}\StringTok{ }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(DT[,m] }\OperatorTok{>}\DecValTok{0}\NormalTok{))}
  \CommentTok{# D_ml: Number of documents that contain both of word m and word l}
  \CommentTok{# D_l: Number of documents that contain word l}
  \CommentTok{# D_m: Number of documents that contain word m }
  
  \KeywordTok{return}\NormalTok{(}\KeywordTok{log}\NormalTok{( (D_ml }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{/}\StringTok{ }\NormalTok{D_l))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Using the same ten models generated in the previous section, plot the
LCP coherence values (\texttt{compute\_c} function).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{top_words =}\StringTok{ }\DecValTok{15}
\NormalTok{LCPResults =}\StringTok{ }\KeywordTok{sapply}\NormalTok{(LDA_models, }\ControlFlowTok{function}\NormalTok{(LDA_model) }\KeywordTok{compute_c}\NormalTok{(LDA_model}\OperatorTok{@}\NormalTok{beta,yelp_matrix[}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{),],top_words, }\StringTok{"LCP"}\NormalTok{))}
\NormalTok{LCPResults =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{),LCPResults)}
\KeywordTok{plot}\NormalTok{(LCPResults, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"Number of topics"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Log Likelihood"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Intrinsic Topic Coherence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{topic_model_files/figure-latex/unnamed-chunk-11-1.pdf}

While these methods can take the average coherence of all topics,
another common way people use them is to take the most and least K
coherent topics from different models and compare. You can optionally
try this by setting the top\_K parameter of \texttt{compute\_c} to 5.

\hypertarget{document-clustering}{%
\subsection{Document Clustering}\label{document-clustering}}

First build ten LDA models (k from 10 to 100 by 10) over all the
documents in the Yelp dataset. For the sake of time, let's restrict it
to the first 10,000 documents of Yelp and set the iterations back down
to 10.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{yelp <-}\StringTok{ }\NormalTok{yelp_matrix[}\DecValTok{1}\OperatorTok{:}\DecValTok{10000}\NormalTok{,]}
\NormalTok{LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{))\{}
\NormalTok{  model <-}\StringTok{ }\KeywordTok{LDA}\NormalTok{(yelp, }\DataTypeTok{control=}\KeywordTok{list}\NormalTok{(}\DataTypeTok{iter=}\DecValTok{10}\NormalTok{, }\DataTypeTok{seed=}\NormalTok{SEED), }\DataTypeTok{k=}\NormalTok{k, }\DataTypeTok{method=}\StringTok{"Gibbs"}\NormalTok{)}
\NormalTok{  LDA_models <-}\StringTok{ }\KeywordTok{c}\NormalTok{(LDA_models, model)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Next, find the topic (cluster) each document belongs to by selecting the
topic that has the highest posterior probability for that document. You
can find these probabilities in the \texttt{@gamma} matrix of the LDA
object. Do this for each of the ten LDA models. You can store the
results in a list of 10 vectors, where each vector contains 10k
elements. Meaning, \texttt{clusters{[}{[}2{]}{]}{[}3{]}} would give you
the cluster ID of the third document in the second LDA model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{clusters <-}\StringTok{ }\KeywordTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(model }\ControlFlowTok{in}\NormalTok{ LDA_models)\{}
\NormalTok{  cluster1 <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\KeywordTok{apply}\NormalTok{(model}\OperatorTok{@}\NormalTok{gamma, }\DecValTok{1}\NormalTok{, }\DataTypeTok{FUN=}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sort}\NormalTok{(x, }\DataTypeTok{decreasing=}\NormalTok{T, }\DataTypeTok{index.return=}\NormalTok{T)}\OperatorTok{$}\NormalTok{ix[}\DecValTok{1}\NormalTok{]))}
\NormalTok{  clusters <-}\StringTok{ }\KeywordTok{c}\NormalTok{(clusters, cluster1)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We are assuming each document has a label, that is how we evaluate the
document clusters. For illustration purposes, we will just randomly
generate these labels, though in practice you would need a training set
that is labeled already. Let's say there are ten labels total.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labels <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{each=}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, create a function called purity, that takes a cluster assignment
and vector of labels as input and calculates the purity of that
clustering assignment. An explanation and example of how purity is
calculated can be found
\href{http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html}{here}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HERE}
\NormalTok{purity <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(cluster, labels)\{}
\NormalTok{  counterValue <-}\StringTok{ }\DecValTok{0}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{unique}\NormalTok{(cluster))\{}
\NormalTok{    documents <-}\StringTok{ }\KeywordTok{which}\NormalTok{(cluster}\OperatorTok{==}\NormalTok{i)}
\NormalTok{    documentLabels <-}\StringTok{ }\NormalTok{labels[documents]}
\NormalTok{    label_counterValues <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(}\KeywordTok{unique}\NormalTok{(documentLabels), }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{length}\NormalTok{(}\KeywordTok{which}\NormalTok{(documentLabels}\OperatorTok{==}\NormalTok{x)))}
\NormalTok{    counterValue <-}\StringTok{ }\NormalTok{counterValue }\OperatorTok{+}\StringTok{ }\KeywordTok{max}\NormalTok{(label_counterValues)}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{ (counterValue}\OperatorTok{/}\KeywordTok{length}\NormalTok{(cluster))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, create a plot showing the purity for each of the ten k-values
from the trained LDA models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# YOUR CODE HEREs}
\NormalTok{purity1 <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(clusters, purity, }\DataTypeTok{labels=}\NormalTok{labels)}
\NormalTok{purity2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{10}\NormalTok{), purity1)}
\KeywordTok{plot}\NormalTok{(purity2, }\DataTypeTok{type=}\StringTok{"o"}\NormalTok{, }\DataTypeTok{col=}\StringTok{"red"}\NormalTok{, }\DataTypeTok{xlab=}\StringTok{"Number of Topics"}\NormalTok{, }\DataTypeTok{ylab=}\StringTok{"Purity"}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Document Clustering Purities"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{topic_model_files/figure-latex/unnamed-chunk-16-1.pdf}

\end{document}
