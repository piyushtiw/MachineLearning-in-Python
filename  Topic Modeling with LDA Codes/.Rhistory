points(8, top100Values[8], col="red")
ap_top_terms
top100Words[1:8]
ap_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered()
library(topicmodels)
library(ggplot2)
library(tm)
library(stringr)
SEED = 3456
data("AssociatedPress", package="topicmodels")
LDA_model <- LDA(AssociatedPress, control=list(iter=20, seed=SEED), k=20, method="Gibbs")
ProbOfEachWord <- apply(exp(LDA_model@beta), 2, max)
top100 <- sort(ProbOfEachWord, decreasing=T, index.return=T)
top100Values <- top100$x[1:100]
top100Words <- LDA_model@terms[top100$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(8, top100Values[8], col="red")
# YOUR CODE HERE
top100Words[1:8]
train <- AssociatedPress[1:1500,]
test <- AssociatedPress[-(1:1500),]
LDA_models <- c()
for(k in seq(20,200,20)){
model <- LDA(train, control=list(iter=20, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
perplex <- sapply(LDA_models, function(x) perplexity(x, test))
plot(perplex, type="o", col="blue", axes=F, ann=F)
axis(1,  at=1:10, lab=seq(20,200,20))
axis(2,  at=seq(2500,3500,100), lab=seq(2500,3500,100))
title(xlab="Number of topics", ylab="Perplexity", main="Perplexity growth")
yelp = read.csv("yelp.txt", header=FALSE, quote="", sep="|")
yelp_text =  as.list(levels(yelp$V1))
clean_yelp = gsub("&amp", "", yelp_text)
clean_yelp = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_yelp)
clean_yelp = gsub("@\\w+", "", clean_yelp)
clean_yelp = gsub("[[:punct:]]", "", clean_yelp)
clean_yelp = gsub("[[:digit:]]", "", clean_yelp)
clean_yelp = gsub("http\\w+", "", clean_yelp)
clean_yelp = gsub("[ \t]{2,}", " ", clean_yelp)
clean_yelp = gsub("[ \n]{2,}", " ", clean_yelp)
clean_yelp = gsub("^\\s+|\\s+$", "", clean_yelp)
clean_yelp <- str_replace_all(clean_yelp," "," ")
clean_yelp <- iconv(clean_yelp, 'UTF-8', 'ASCII',sub = "")
yelp_Corpus <- Corpus(VectorSource(clean_yelp))
yelp_matrix <- DocumentTermMatrix(yelp_Corpus,control = list(tolower = TRUE, sparse=TRUE, stemming = TRUE, stopwords = TRUE, minWordLength = 3,removeNumbers = TRUE, removePunctuation = TRUE))
yelp_matrix <- removeSparseTerms(yelp_matrix, 0.995)
rowTotals <- apply(yelp_matrix , 1, sum)
yelp_matrix   <- yelp_matrix[rowTotals> 0, ]  #removing documents that became empty after processing
yelp_matrix <- as.matrix(yelp_matrix)
NPMI = function(DT, m,l){
number_of_documents = dim(DT)[1]
p_ml = length(which(DT[,l] >0 & DT[,m] >0))  / (number_of_documents * number_of_documents)
p_l = length(which(DT[,l] >0))/number_of_documents
p_m = length(which(DT[,m] >0))/number_of_documents
# p_ml: probability of word m and word l both appears in a document
# p_l: probability of word l appears in a document
# p_m: probability of word m appears in a document
if (p_ml==0)
return(0)
else
return( log( p_ml  / (p_l * p_m)) / -log(p_ml) )
}
compute_c <- function(LDA_model, dataset,  top_N, method=c("LCP", "NPMI"), top_K= 0){
c = list()
if(method == "LCP")
method = LCP
else
method = NPMI
top_words <- apply(t(LDA_model), 2, FUN = function(x) sort(x, decreasing = T,index.return = T)$ix[1:top_N]) #find top N words
#the following nested for-loop computes NPMI or LCP for all word pairs in top N for all topics
for( i in 1:dim(top_words)[2]){
temp_c = 0
for( m in 2:top_N){
for(l in 1: (m-1)){
temp_c = temp_c + method(dataset,top_words[m,i],top_words[l,i])
}
}
c[[i]] = temp_c
}
c = as.numeric(c)
if(top_K == 0)
return( sum(c)/dim(LDA_model)[1])
else
return( sum(sort(c, decreasing = T,index.return = T)$x[1:top_K]) / top_K  )
}
train <- yelp_matrix[1:1000,]
test <- yelp_matrix[-(1:1000),]
LDA_models <- c()
for(k in seq(10,100,10)){
model <- LDA(train, control=list(iter=100, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
top_words = 15
NPMI_results = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta, test, top_words, "NPMI"))
NPMI_results = data.frame(seq(10,100,10), NPMI_results)
plot(NPMI_results, type="o", col="blue", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
LCP = function(DT, m,l ){
D_ml = length(which(DT[,m] >0 & DT[,l] >0))
D_l = length(which(DT[,l] >0))
D_m = length(which(DT[,m] >0))
# D_ml: Number of documents that contain both of word m and word l
# D_l: Number of documents that contain word l
# D_m: Number of documents that contain word m
return(log( (D_ml + 1) / D_l))
}
LCPResults = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
LCPResults = data.frame(seq(10,100,10),LCPResults)
plot(LCPResults, type="o", col="blue",xlab="Number of topics", ylab="Log Likelihood", main="Intrinsic Topic Coherence")
clusters <- list()
for(model in LDA_models){
cluster1 <- list(apply(model@gamma, 1, FUN=function(x) sort(x, decreasing=T, index.return=T)$ix[1]))
clusters <- c(clusters, cluster1)
}
labels <- rep(1:10, each=1000)
purity <- function(cluster, labels){
counterValue <- 0
for (i in unique(cluster)){
documents <- which(cluster==i)
documentLabels <- labels[documents]
label_counterValues <- sapply(unique(documentLabels), function(x) length(which(documentLabels==x)))
counterValue <- counterValue + max(label_counterValues)
}
return (counterValue/length(cluster))
}
plot(purity, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities")
purity1 <- sapply(clusters, purity, labels=labels)
purity = data.frame(seq(10,100,10), purity1)
plot(purity, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities", labels="No")
purity1 <- sapply(clusters, purity, labels=labels)
purity <- function(cluster, labels){
counterValue <- 0
for (i in unique(cluster)){
documents <- which(cluster==i)
documentLabels <- labels[documents]
label_counterValues <- sapply(unique(documentLabels), function(x) length(which(documentLabels==x)))
counterValue <- counterValue + max(label_counterValues)
}
return (counterValue/length(cluster))
}
purity1 <- sapply(clusters, purity, labels=labels)
plot(purity, type="o", col="red", xlab="Number of Topics", ylab="Purity")
plot(purity, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities", labels=labels)
labels
plot(purity, labels=labels, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities")
plot(purity,type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities")
purity
purity2 = data.frame(seq(10,100,10), purity1)
purity2
purity1 <- sapply(clusters, purity, labels=labels)
purity2 = data.frame(seq(10,100,10), purity1)
plot(purity2, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities")
purity1
top100
top100 <- sort(ProbOfEachWord, decreasing=T)
top100Values <- top100$x[1:100]
top100 <- sort(ProbOfEachWord, decreasing=T, index.return=T)
top100Values <- top100$x[1:100]
top100Values
top100Words
ProbOfEachWord
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top <- head(sortedByProbWords, 100)
topWords <- LDA_model@terms[top100$ix[1:100]]
topWords
class(wordsProb)
class(LDA_model)
plot(1:100, top, "o", xlab="Word", ylab="Probability")
plot(1:100, top, "o", xlab="Word", ylab="Max Probability")
top
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top <- head(sortedByProbWords, 100)
topWords <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top, "o", xlab="Word", ylab="Max Probability")
top
wordsProb <- apply(exp(LDA_model@beta), 2, max)
top100 <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- top100$x[1:100]
top100Words <- LDA_model@terms[top100$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(8, top100Values[8], col="red")
# YOUR CODE HERE
top100Words[1:8]
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- sortedByProbWords$x[1:100]
top100Words <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(8, top100Values[8], col="red")
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- head(sortedByProbWords, 100)
top100Words <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- sortedByProbWords$x[1:100]
top100Words <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(10, top100Values[10], col="red")
# YOUR CODE HERE
top100Words[1:10]
top100Words
train <- AssociatedPress[1:1500,]
test <- AssociatedPress[-(1:1500),]
LDA_models <- c()
for(k in seq(20,200,20)){
model <- LDA(train, control=list(iter=20, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
perplex <- sapply(LDA_models, function(x) perplexity(x, test))
plot(perplex, type="o", col="red", axes=F, ann=F)
axis(1,  at=1:10, lab=seq(20,200,20))
axis(2,  at=seq(2500,3500,100), lab=seq(2500,3500,100))
title(xlab="Number of topics", ylab="Perplexity", main="Perplexity growth")
train <- yelp_matrix[1:1000,]
test <- yelp_matrix[-(1:1000),]
LDA_models <- c()
for(k in seq(10,100,10)){
model <- LDA(train, control=list(iter=100, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
top_n = 15
NPMI_results = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta, test, top_n, "NPMI"))
NPMI_results = data.frame(seq(10,100,10), NPMI_results)
plot(NPMI_results, type="o", col="blue", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
lot(NPMI_results, type="o", col="red", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
top_n = 15
NPMI_results = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta, test, top_n, "NPMI"))
NPMI_results = data.frame(seq(10,100,10), NPMI_results)
plot(NPMI_results, type="o", col="red", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
LCPResults = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
LCPResults = data.frame(seq(10,100,10),LCPResults)
plot(LCPResults, type="o", col="red",xlab="Number of topics", ylab="Log Likelihood", main="Intrinsic Topic Coherence")
yelp <- yelp_matrix[1:10000,]
LDA_models <- c()
for(k in seq(10,100,10)){
model <- LDA(yelp, control=list(iter=10, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
clusters <- list()
for(model in LDA_models){
cluster1 <- list(apply(model@gamma, 1, FUN=function(x) sort(x, decreasing=T, index.return=T)$ix[1]))
clusters <- c(clusters, cluster1)
}
labels <- rep(1:10, each=1000)
purity <- function(cluster, labels){
counterValue <- 0
for (i in unique(cluster)){
documents <- which(cluster==i)
documentLabels <- labels[documents]
label_counterValues <- sapply(unique(documentLabels), function(x) length(which(documentLabels==x)))
counterValue <- counterValue + max(label_counterValues)
}
return (counterValue/length(cluster))
}
purity1 <- sapply(clusters, purity, labels=labels)
purity2 = data.frame(seq(10,100,10), purity1)
plot(purity2, type="o", col="red", xlab="Number of Topics", ylab="Purity", main="Document Clustering Purities")
library(topicmodels)
library(ggplot2)
library(tm)
library(stringr)
SEED = 3456
data("AssociatedPress", package="topicmodels")
LDA_model <- LDA(AssociatedPress, control=list(iter=20, seed=SEED), k=20, method="Gibbs")
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- sortedByProbWords$x[1:100]
top100Words <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(10, top100Values[10], col="red")
top100Words[1:10]
train <- AssociatedPress[1:1500,]
test <- AssociatedPress[-(1:1500),]
LDA_models <- c()
for(k in seq(20,200,20)){
model <- LDA(train, control=list(iter=20, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
perplex <- sapply(LDA_models, function(x) perplexity(x, test))
plot(perplex, type="o", col="red", axes=F, ann=F)
axis(1,  at=1:10, lab=seq(20,200,20))
axis(2,  at=seq(2500,3500,100), lab=seq(2500,3500,100))
title(xlab="Number of topics", ylab="Perplexity", main="Perplexity growth")
yelp = read.csv("yelp.txt", header=FALSE, quote="", sep="|")
yelp_text =  as.list(levels(yelp$V1))
clean_yelp = gsub("&amp", "", yelp_text)
clean_yelp = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_yelp)
clean_yelp = gsub("@\\w+", "", clean_yelp)
clean_yelp = gsub("[[:punct:]]", "", clean_yelp)
clean_yelp = gsub("[[:digit:]]", "", clean_yelp)
clean_yelp = gsub("http\\w+", "", clean_yelp)
clean_yelp = gsub("[ \t]{2,}", " ", clean_yelp)
clean_yelp = gsub("[ \n]{2,}", " ", clean_yelp)
clean_yelp = gsub("^\\s+|\\s+$", "", clean_yelp)
clean_yelp <- str_replace_all(clean_yelp," "," ")
clean_yelp <- iconv(clean_yelp, 'UTF-8', 'ASCII',sub = "")
yelp_Corpus <- Corpus(VectorSource(clean_yelp))
yelp_matrix <- DocumentTermMatrix(yelp_Corpus,control = list(tolower = TRUE, sparse=TRUE, stemming = TRUE, stopwords = TRUE, minWordLength = 3,removeNumbers = TRUE, removePunctuation = TRUE))
yelp_matrix <- removeSparseTerms(yelp_matrix, 0.995)
rowTotals <- apply(yelp_matrix , 1, sum)
yelp_matrix   <- yelp_matrix[rowTotals> 0, ]  #removing documents that became empty after processing
yelp_matrix <- as.matrix(yelp_matrix)
NPMI = function(DT, m,l){
number_of_documents = dim(DT)[1]
p_ml = length(which(DT[,l] >0 & DT[,m] >0))  / (number_of_documents * number_of_documents)
p_l = length(which(DT[,l] >0))/number_of_documents
p_m = length(which(DT[,m] >0))/number_of_documents
# p_ml: probability of word m and word l both appears in a document
# p_l: probability of word l appears in a document
# p_m: probability of word m appears in a document
if (p_ml==0)
return(0)
else
return( log( p_ml  / (p_l * p_m)) / -log(p_ml) )
}
compute_c <- function(LDA_model, dataset,  top_N, method=c("LCP", "NPMI"), top_K= 0){
c = list()
if(method == "LCP")
method = LCP
else
method = NPMI
top_words <- apply(t(LDA_model), 2, FUN = function(x) sort(x, decreasing = T,index.return = T)$ix[1:top_N]) #find top N words
#the following nested for-loop computes NPMI or LCP for all word pairs in top N for all topics
for( i in 1:dim(top_words)[2]){
temp_c = 0
for( m in 2:top_N){
for(l in 1: (m-1)){
temp_c = temp_c + method(dataset,top_words[m,i],top_words[l,i])
}
}
c[[i]] = temp_c
}
c = as.numeric(c)
if(top_K == 0)
return( sum(c)/dim(LDA_model)[1])
else
return( sum(sort(c, decreasing = T,index.return = T)$x[1:top_K]) / top_K  )
}
NPMI = function(DT, m,l){
number_of_documents = dim(DT)[1]
p_ml = length(which(DT[,l] >0 & DT[,m] >0))  / (number_of_documents * number_of_documents)
p_l = length(which(DT[,l] >0))/number_of_documents
p_m = length(which(DT[,m] >0))/number_of_documents
# p_ml: probability of word m and word l both appears in a document
# p_l: probability of word l appears in a document
# p_m: probability of word m appears in a document
if (p_ml==0)
return(0)
else
return( log( p_ml  / (p_l * p_m)) / -log(p_ml) )
}
compute_c <- function(LDA_model, dataset,  top_N, method=c("LCP", "NPMI"), top_K= 0){
c = list()
if(method == "LCP")
method = LCP
else
method = NPMI
top_words <- apply(t(LDA_model), 2, FUN = function(x) sort(x, decreasing = T,index.return = T)$ix[1:top_N]) #find top N words
#the following nested for-loop computes NPMI or LCP for all word pairs in top N for all topics
for( i in 1:dim(top_words)[2]){
temp_c = 0
for( m in 2:top_N){
for(l in 1: (m-1)){
temp_c = temp_c + method(dataset,top_words[m,i],top_words[l,i])
}
}
c[[i]] = temp_c
}
c = as.numeric(c)
if(top_K == 0)
return( sum(c)/dim(LDA_model)[1])
else
return( sum(sort(c, decreasing = T,index.return = T)$x[1:top_K]) / top_K  )
}
train <- yelp_matrix[1:1000,]
test <- yelp_matrix[-(1:1000),]
LDA_models <- c()
for(k in seq(10,100,10)){
model <- LDA(train, control=list(iter=100, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
top_n = 15
NPMI = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta, test, top_n, "NPMI"))
NPMI = data.frame(seq(10,100,10), NPMI)
plot(NPMI, type="o", col="red", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
LCP = function(DT, m,l ){
D_ml = length(which(DT[,m] >0 & DT[,l] >0))
D_l = length(which(DT[,l] >0))
D_m = length(which(DT[,m] >0))
# D_ml: Number of documents that contain both of word m and word l
# D_l: Number of documents that contain word l
# D_m: Number of documents that contain word m
return(log( (D_ml + 1) / D_l))
}
LCP = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
LCPResults = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
library(topicmodels)
library(ggplot2)
library(tm)
library(stringr)
SEED = 3456
data("AssociatedPress", package="topicmodels")
LDA_model <- LDA(AssociatedPress, control=list(iter=20, seed=SEED), k=20, method="Gibbs")
wordsProb <- apply(exp(LDA_model@beta), 2, max)
sortedByProbWords <- sort(wordsProb, decreasing=T, index.return=T)
top100Values <- sortedByProbWords$x[1:100]
top100Words <- LDA_model@terms[sortedByProbWords$ix[1:100]]
plot(1:100, top100Values, "o", xlab="Word", ylab="Max Probability")
points(10, top100Values[10], col="red")
top100Words[1:10]
train <- AssociatedPress[1:1500,]
test <- AssociatedPress[-(1:1500),]
LDA_models <- c()
for(k in seq(20,200,20)){
model <- LDA(train, control=list(iter=20, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
perplex <- sapply(LDA_models, function(x) perplexity(x, test))
plot(perplex, type="o", col="red", axes=F, ann=F)
axis(1,  at=1:10, lab=seq(20,200,20))
axis(2,  at=seq(2500,3500,100), lab=seq(2500,3500,100))
title(xlab="Number of topics", ylab="Perplexity", main="Perplexity growth")
yelp = read.csv("yelp.txt", header=FALSE, quote="", sep="|")
yelp_text =  as.list(levels(yelp$V1))
clean_yelp = gsub("&amp", "", yelp_text)
clean_yelp = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_yelp)
clean_yelp = gsub("@\\w+", "", clean_yelp)
clean_yelp = gsub("[[:punct:]]", "", clean_yelp)
clean_yelp = gsub("[[:digit:]]", "", clean_yelp)
clean_yelp = gsub("http\\w+", "", clean_yelp)
clean_yelp = gsub("[ \t]{2,}", " ", clean_yelp)
clean_yelp = gsub("[ \n]{2,}", " ", clean_yelp)
clean_yelp = gsub("^\\s+|\\s+$", "", clean_yelp)
clean_yelp <- str_replace_all(clean_yelp," "," ")
clean_yelp <- iconv(clean_yelp, 'UTF-8', 'ASCII',sub = "")
yelp_Corpus <- Corpus(VectorSource(clean_yelp))
yelp_matrix <- DocumentTermMatrix(yelp_Corpus,control = list(tolower = TRUE, sparse=TRUE, stemming = TRUE, stopwords = TRUE, minWordLength = 3,removeNumbers = TRUE, removePunctuation = TRUE))
yelp_matrix <- removeSparseTerms(yelp_matrix, 0.995)
rowTotals <- apply(yelp_matrix , 1, sum)
yelp_matrix   <- yelp_matrix[rowTotals> 0, ]  #removing documents that became empty after processing
yelp_matrix <- as.matrix(yelp_matrix)
NPMI = function(DT, m,l){
number_of_documents = dim(DT)[1]
p_ml = length(which(DT[,l] >0 & DT[,m] >0))  / (number_of_documents * number_of_documents)
p_l = length(which(DT[,l] >0))/number_of_documents
p_m = length(which(DT[,m] >0))/number_of_documents
# p_ml: probability of word m and word l both appears in a document
# p_l: probability of word l appears in a document
# p_m: probability of word m appears in a document
if (p_ml==0)
return(0)
else
return( log( p_ml  / (p_l * p_m)) / -log(p_ml) )
}
compute_c <- function(LDA_model, dataset,  top_N, method=c("LCP", "NPMI"), top_K= 0){
c = list()
if(method == "LCP")
method = LCP
else
method = NPMI
top_words <- apply(t(LDA_model), 2, FUN = function(x) sort(x, decreasing = T,index.return = T)$ix[1:top_N]) #find top N words
#the following nested for-loop computes NPMI or LCP for all word pairs in top N for all topics
for( i in 1:dim(top_words)[2]){
temp_c = 0
for( m in 2:top_N){
for(l in 1: (m-1)){
temp_c = temp_c + method(dataset,top_words[m,i],top_words[l,i])
}
}
c[[i]] = temp_c
}
c = as.numeric(c)
if(top_K == 0)
return( sum(c)/dim(LDA_model)[1])
else
return( sum(sort(c, decreasing = T,index.return = T)$x[1:top_K]) / top_K  )
}
train <- yelp_matrix[1:1000,]
test <- yelp_matrix[-(1:1000),]
LDA_models <- c()
for(k in seq(10,100,10)){
model <- LDA(train, control=list(iter=100, seed=SEED), k=k, method="Gibbs")
LDA_models <- c(LDA_models, model)
}
top_n = 15
NPMI_results = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta, test, top_n, "NPMI"))
NPMI_results = data.frame(seq(10,100,10), NPMI_results)
plot(NPMI_results, type="o", col="red", xlab="Number of topics", ylab="NPMI", main="Extrinsic topic coherence")
LCP = function(DT, m,l ){
D_ml = length(which(DT[,m] >0 & DT[,l] >0))
D_l = length(which(DT[,l] >0))
D_m = length(which(DT[,m] >0))
# D_ml: Number of documents that contain both of word m and word l
# D_l: Number of documents that contain word l
# D_m: Number of documents that contain word m
return(log( (D_ml + 1) / D_l))
}
top_words
LCPResults = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
LCPResults = sapply(LDA_models, function(LDA_model) compute_c(LDA_model@beta,yelp_matrix[-(1:1000),],top_words, "LCP"))
LCPResults = data.frame(seq(10,100,10),LCPResults)
plot(LCPResults, type="o", col="red",xlab="Number of topics", ylab="Log Likelihood", main="Intrinsic Topic Coherence")
