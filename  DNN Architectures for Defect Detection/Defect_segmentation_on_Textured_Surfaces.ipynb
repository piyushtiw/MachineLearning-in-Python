{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_x7KZaVdWQMP",
    "outputId": "a19311ea-4c87-47ce-fe18-b8bcd540144a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "1HkebwbOWSpQ",
    "outputId": "1550a578-9b30-4f73-82d4-2fdf39e6a8c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  optical_data.zip\n",
      "replace data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
      "  inflating: data/.DS_Store          \n",
      "replace __MACOSX/data/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
      "  inflating: __MACOSX/data/._.DS_Store  \n",
      "replace data/Class3/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
      "  inflating: data/Class3/.DS_Store   \n",
      "replace __MACOSX/data/Class3/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
      "  inflating: __MACOSX/data/Class3/._.DS_Store  \n",
      "replace data/Class3/Test/0298.PNG? [y]es, [n]o, [A]ll, [N]one, [r]ename: Y\n",
      "  inflating: data/Class3/Test/0298.PNG  \n",
      "replace __MACOSX/data/Class3/Test/._0298.PNG? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
     ]
    }
   ],
   "source": [
    "!cp gdrive/My\\ Drive/optical_data.zip .\n",
    "!unzip optical_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8pO-Pc2gWghq",
    "outputId": "f4d07da5-6283-4367-93aa-ef0c555d221f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1\tClass2\tClass3\tClass4\tClass5\tClass6\tClass7\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BWcOx9_CZueT",
    "outputId": "96ab257e-fa49-4973-9c38-a865cf1a0b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q4GiTvr7ZyiQ"
   },
   "outputs": [],
   "source": [
    "# Not all data in the dataset has defects. We only use the images which have defects\n",
    "# This function takes the dataset_type as a parameter. You can pass \"Train\" or \"Test\"\n",
    "# as argument to get the appropriate dataset\n",
    "def load_data(dataset_type=\"Train\"):\n",
    "    file_list = {}\n",
    "    defect_map = {}\n",
    "    file_name = []\n",
    "    file_mask = []\n",
    "    count = 0\n",
    "    num_classes = 6\n",
    "\n",
    "    data_dir = \"data\"\n",
    "    for x in range(1, num_classes + 1):\n",
    "        path = os.path.join(os.path.join(data_dir, \"Class\" + str(x)), dataset_type)\n",
    "        df = pd.read_fwf(path + \"/Label/Labels.txt\")\n",
    "        count = 0\n",
    "        for i in range(0, len(df)):\n",
    "            curr_file = path + \"/\" + str(df.iloc[i][2])\n",
    "            if (df.iloc[i][1] == 1):\n",
    "                file_list[curr_file] = path + \"/Label/\" + str(df.iloc[i][4])\n",
    "                defect_map[curr_file] = 1\n",
    "            else:\n",
    "                fnametest = str(df.iloc[i][2]).split(\".\")\n",
    "                file_list[curr_file] = str(path + \"/Label/\" + fnametest[0] + \"_label.PNG\")\n",
    "                defect_map[curr_file] = 0\n",
    "\n",
    "    items = list(file_list.keys())\n",
    "    shuffle(items)\n",
    "    for key in items:\n",
    "        if ((not os.path.exists(key)) or (not os.path.exists(file_list[key]))):\n",
    "            # print (\"Missing mask for \", key)\n",
    "            continue\n",
    "\n",
    "        if defect_map[key] == 1:\n",
    "            file_name.append(key)\n",
    "            file_mask.append(file_list[key])\n",
    "        elif count < 80 * num_classes:\n",
    "            file_name.append(key)\n",
    "            file_mask.append(file_list[key])\n",
    "            count = count + 1\n",
    "\n",
    "    return file_name, file_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DxL-2LcEZ44n"
   },
   "outputs": [],
   "source": [
    "# This is generator class to process data in batches and send them for training\n",
    "class Surface_Generator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, image_filenames, labels, batch_size, test=False):\n",
    "        self.image_filenames, self.labels = image_filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    # return the total number of batches you have i.e., total_files/batch_size\n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE\n",
    "        # return ...\n",
    "        return int(np.floor(len(self.image_filenames) / self.batch_size)) + 1\n",
    "\n",
    "    # this function is called for every mini-batch to get the images/masks for that mini-batch\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        image_arr = []\n",
    "        mask_arr = []\n",
    "        # Open a batch of images and their corresponding masks using cv2.imread\n",
    "        # resize them to 512x512x1 and return an np.array of images and masks\n",
    "        # YOUR CODE HERE\n",
    "        for x in batch_x:\n",
    "          image_arr.append(resize(cv2.imread(x,0),(512,512,1)))\n",
    "        for y in batch_y:\n",
    "          mask_arr.append(resize(cv2.imread(y,0),(512,512,1)))\n",
    "\n",
    "        return np.array(image_arr).astype(np.float32), np.array(mask_arr).astype(np.float32)\n",
    "    \n",
    "    # for testing we need to get the list of all true masks\n",
    "    # this function should return all the labels in the dataset set \n",
    "    # we will call this function only for the \"Test\" dataset\n",
    "    def get_all_masks(self):\n",
    "        mask_arr = []\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        for x in self.labels:\n",
    "          mask_arr.append(resize(cv2.imread(x,0),(512,512,1)))\n",
    "\n",
    "        return np.array(mask_arr).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "eF6x2-4NaJbk"
   },
   "outputs": [],
   "source": [
    "# Since we already have a split for training and test set,\n",
    "# we just need to split training set to get a validation set\n",
    "\n",
    "# Load training data\n",
    "X, Y = load_data(\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QZDc5jATbrEC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2uga0pE4b8Ae",
    "outputId": "59d40d45-b108-4648-834a-bdc57542cbb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "print(\"done loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "bkjd2CFPZwws"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dv5XHHo3cJVJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2J43kJu_ao9V",
    "outputId": "886e51bb-387a-4851-a22e-39de7efeb8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740,) (740,) (186,) (186,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ISB0ZvTKav92",
    "outputId": "b88cc007-37a9-4d24-894b-073e7ee70d49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4675"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free memory\n",
    "import gc\n",
    "del X\n",
    "del Y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GcpudYjibJqV"
   },
   "outputs": [],
   "source": [
    "# Dice Coefficient metric\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "# Dice Coefficient loss\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FKtfhAjoiUIT"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Dropout, Activation, BatchNormalization, UpSampling2D, Concatenate\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "# Create a 2D convolution block. We will use multiple instances of this block to build our U-net model\n",
    "# This block will contain two layers. \n",
    "# Each layer will be a Convolution operation followed by batch normalization with relu activation \n",
    "def conv2d_block(input_tensor, n_filters, kernel_size):\n",
    "    # first layer\n",
    "    # Create a Conv2D layer with n_filters and a kernel of dimension : kernel_size x kernel_size. \n",
    "    # Use same padding and he_normal initializer\n",
    "    # YOUR CODE HERE\n",
    "    # x = ...\n",
    "    x = Conv2D(n_filters, (kernel_size, kernel_size), kernel_initializer=keras.initializers.he_normal(seed=None), padding='same')(input_tensor)\n",
    "    \n",
    "    # add a BatchNormalization layer\n",
    "    # YOUR CODE HERE\n",
    "    # x = ...\n",
    "    x = BatchNormalization()(x)\n",
    "        \n",
    "    # Add a relu non-linearity (keras.layers.Activation)\n",
    "    # YOUR CODE HERE\n",
    "    # x = ...\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # second layer\n",
    "    # repeat the above steps (Conv + batchnorm + relu) taking the output of relu layer as input for this convolutional layer\n",
    "    # YOUR CODE HERE\n",
    "    # x = ...\n",
    "    x = Conv2D(n_filters, (kernel_size, kernel_size), kernel_initializer=keras.initializers.he_normal(seed=None), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # return the output tensor\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_unet_model(n_filters=16, dropout_prob=0.5, kernel_size=3):\n",
    "    input_img = Input((512, 512, 1))\n",
    "    \n",
    "    # contracting path\n",
    "    # create a convolutional block with input_img as the input tensor and n_filters\n",
    "    # YOUR CODE HERE\n",
    "    # c1 = ...\n",
    "    c1 = conv2d_block(input_img, n_filters, kernel_size)\n",
    "\n",
    "    # apply a 2d maxpooling with a pool size of 2x2\n",
    "    # YOUR CODE HERE\n",
    "    # p1 = ...\n",
    "    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "\n",
    "    # add a dropout. Since this the input, set the dropout rate to 0.5 * dropout_prob\n",
    "    # YOUR CODE HERE\n",
    "    # p1 = ...\n",
    "    p1 = Dropout(0.5*dropout_prob)(p1)\n",
    "\n",
    "\n",
    "    # create another convolutional block. this time use p1 as input tensor and twice the n_filters\n",
    "    # repeat the same maxpool and dropout but set dropout rate to dropout_prob this time\n",
    "    # YOUR CODE HERE\n",
    "    # c2 = ...\n",
    "    # p2 = ...\n",
    "    # p2 = ...\n",
    "    c2 = conv2d_block(p1, 2*n_filters, kernel_size)\n",
    "    p2 = MaxPooling2D(pool_size=(2, 2))(c2)\n",
    "    p2 = Dropout(dropout_prob)(p2)\n",
    "\n",
    "    # create another block with maxpool and dropout with 4 x n_filters\n",
    "    # YOUR CODE HERE\n",
    "    # c3 = ...\n",
    "    # p3 = ...\n",
    "    # p3 = ...\n",
    "    c3 = conv2d_block(p2, 4*n_filters, kernel_size)\n",
    "    p3 = MaxPooling2D(pool_size=(2, 2))(c3)\n",
    "    p3 = Dropout(dropout_prob)(p3)\n",
    "\n",
    "    # create another block with maxpool and dropout with 8 x n_filters\n",
    "    # YOUR CODE HERE\n",
    "    # c4 = ...\n",
    "    # p4 = ...\n",
    "    # p4 = ...\n",
    "    c4 = conv2d_block(p3, 8*n_filters, kernel_size)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "    p4 = Dropout(dropout_prob)(p4)\n",
    "\n",
    "\n",
    "    # This is the layer where we combine the contractive and expansive paths\n",
    "    # create a convolutional block with 16 x n_filters. No pooling/dropout this time\n",
    "    # YOUR CODE HERE\n",
    "    # c5 = ...\n",
    "    c5 = conv2d_block(p4, 16*n_filters,kernel_size)\n",
    "\n",
    "    \n",
    "    # Expansive path\n",
    "\n",
    "    # We will create a similar structure as the contracting path but instead of \n",
    "    # convolutional operation, we will use Deconvolution operations\n",
    "\n",
    "    # Create a Conv2DTranspose layer (deconvolution) with 8 x n_filters, kernel_size, \n",
    "    # 2x2 strides and same padding\n",
    "    # YOUR CODE HERE\n",
    "    # u6 = ...\n",
    "    u6 = Conv2DTranspose(8*n_filters,kernel_size= kernel_size, padding='same' , strides=(2,2))(c5)\n",
    "\n",
    "    # Concatenate u6 and c4 using keras.layers.concatenate\n",
    "    # YOUR CODE HERE\n",
    "    # u6 = ...\n",
    "    u6 = concatenate([u6, c4])\n",
    "    \n",
    "    # dropout\n",
    "    # YOUR CODE HERE\n",
    "    # u6 = ...\n",
    "    u6 = Dropout(dropout_prob)(u6)\n",
    "    \n",
    "    # create a convolutional block with 8 x n_filters\n",
    "    # YOUR CODE HERE\n",
    "    # c6 = ...\n",
    "    c6 = conv2d_block(u6, 8*n_filters, kernel_size)\n",
    "    \n",
    "\n",
    "    # Create a similar module as previous, deconv, concatenate, dropout, conv2d_block\n",
    "    # Please ensure that the number of filters you use match the n_filters of \n",
    "    # the layer you are concatenating with\n",
    "    # YOUR CODE HERE\n",
    "    # u7 = ...\n",
    "    # u7 = ...\n",
    "    # u7 = ...\n",
    "    # c7 = ...\n",
    "    u7 = Conv2DTranspose(4*n_filters, kernel_size=kernel_size, strides=(2,2),  padding='same' )(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout_prob)(u7)\n",
    "    c7 = conv2d_block(u7, 4*n_filters, kernel_size)\n",
    "    \n",
    "    # Create a similar module as previous, deconv, concatenate, dropout, conv2d_block\n",
    "    # YOUR CODE HERE\n",
    "    # u8 = ...\n",
    "    # u8 = ...\n",
    "    # u8 = ...\n",
    "    # c8 = ...\n",
    "    u8 = Conv2DTranspose(2*n_filters,kernel_size=kernel_size, strides=(2,2), padding='same' )(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout_prob)(u8)\n",
    "    c8 = conv2d_block(u8, 2*n_filters, kernel_size)\n",
    "\n",
    "\n",
    "    # Create a similar module as previous, deconv, concatenate, dropout, conv2d_block\n",
    "    # YOUR CODE HERE\n",
    "    # u9 = ...\n",
    "    # u9 = ...\n",
    "    # u9 = ...\n",
    "    # c9 = ...\n",
    "    u9 = Conv2DTranspose(n_filters, kernel_size= kernel_size, strides=(2,2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    u9 = Dropout(dropout_prob)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters, kernel_size)\n",
    "\n",
    "    # apply a 1x1 convolution on c9 to get an output with a single channel\n",
    "    # This is the final model output. We want the pixel values in the mask to be\n",
    "    # either 0 or 1. Choose an activation function which can give values in that\n",
    "    # range.\n",
    "    # YOUR CODE HERE\n",
    "    # outputs = ...\n",
    "    outputs = Conv2D(1,(1,1),activation=\"sigmoid\")(c9)\n",
    "\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "vHm4eLfjeJ7T",
    "outputId": "28b7bbe6-f35f-46bd-8398-2006eb6a6873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unet\"\n",
    "model = get_unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LAZID2Qweqgx",
    "outputId": "8d802800-2e7a-4f7a-88ea-91b7093b0c1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 512, 512, 16) 160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 512, 512, 16) 64          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 512, 512, 16) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 512, 512, 16) 2320        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 512, 512, 16) 64          conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 512, 512, 16) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 256, 256, 16) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256, 256, 16) 0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 32) 4640        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 32) 128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 256, 256, 32) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 256, 256, 32) 9248        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 256, 256, 32) 128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 256, 256, 32) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 128, 128, 32) 0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128, 128, 32) 0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 64) 18496       dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128, 128, 64) 256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 128, 128, 64) 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 128, 64) 36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128, 128, 64) 256         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128, 128, 64) 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 64, 64, 64)   0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 64, 64, 64)   0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 128)  73856       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 64, 64, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 64, 64, 128)  147584      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, 64, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, 64, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 32, 32, 128)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 128)  0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 256)  295168      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 256)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 32, 256)  590080      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 256)  1024        conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 128)  295040      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_5[0][0]         \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 64, 64, 256)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  295040      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 64, 64, 128)  147584      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 64, 64, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 64) 73792       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_6[0][0]         \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128, 128, 128 0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 64) 73792       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 128, 128, 64) 256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 128, 128, 64) 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 128, 64) 36928       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 128, 128, 64) 256         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, 128, 64) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 32) 18464       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_7[0][0]         \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256, 256, 64) 0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 32) 18464       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 256, 256, 32) 128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256, 256, 32) 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 32) 9248        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 256, 256, 32) 128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 256, 256, 32) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 512, 512, 16) 4624        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_8[0][0]         \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 512, 512, 32) 0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 512, 512, 16) 4624        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 512, 512, 16) 64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 512, 512, 16) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 512, 512, 16) 2320        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 512, 512, 16) 64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 512, 512, 16) 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 512, 512, 1)  17          activation_36[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,164,305\n",
      "Trainable params: 2,161,361\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sRJ6OF81frfK"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "# Compile the model\n",
    "model.compile(loss=dice_coef_loss, optimizer=Adam(lr=0.0055), metrics=[dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fef4Gj-JfyNi"
   },
   "outputs": [],
   "source": [
    "num_training_samples = len(X_train)\n",
    "num_validation_samples = len(X_val)\n",
    "\n",
    "training_batch_generator = Surface_Generator(X_train, y_train, batch_size)\n",
    "validation_batch_generator = Surface_Generator(X_val, y_val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yH3j5MIHf5Z8"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(model_name + \"/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor=dice_coef, verbose=1,\n",
    "                               save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "PDgvxBYfgCvU",
    "outputId": "0cb4f189-a4d8-45f2-d97a-71c507115260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "92/92 [==============================] - 60s 657ms/step - loss: 0.9495 - dice_coef: 0.0505 - val_loss: 0.9713 - val_dice_coef: 0.0349\n",
      "Epoch 2/50\n",
      "92/92 [==============================] - 53s 571ms/step - loss: 0.8019 - dice_coef: 0.1981 - val_loss: 0.9841 - val_dice_coef: 0.0346\n",
      "Epoch 3/50\n",
      "92/92 [==============================] - 52s 565ms/step - loss: 0.8985 - dice_coef: 0.1015 - val_loss: 0.9541 - val_dice_coef: 0.0346\n",
      "Epoch 4/50\n",
      "92/92 [==============================] - 52s 567ms/step - loss: 0.8137 - dice_coef: 0.1863 - val_loss: 0.9334 - val_dice_coef: 0.0334\n",
      "Epoch 5/50\n",
      "92/92 [==============================] - 52s 563ms/step - loss: 0.8420 - dice_coef: 0.1580 - val_loss: 0.9897 - val_dice_coef: 0.0183\n",
      "Epoch 6/50\n",
      "92/92 [==============================] - 53s 572ms/step - loss: 0.8595 - dice_coef: 0.1405 - val_loss: 0.5343 - val_dice_coef: 0.2147\n",
      "Epoch 7/50\n",
      "92/92 [==============================] - 53s 573ms/step - loss: 0.7725 - dice_coef: 0.2275 - val_loss: 0.9998 - val_dice_coef: 0.2888\n",
      "Epoch 8/50\n",
      "92/92 [==============================] - 52s 562ms/step - loss: 0.6918 - dice_coef: 0.3082 - val_loss: 0.9888 - val_dice_coef: 0.1903\n",
      "Epoch 9/50\n",
      "92/92 [==============================] - 52s 570ms/step - loss: 0.7374 - dice_coef: 0.2626 - val_loss: 0.5513 - val_dice_coef: 0.3326\n",
      "Epoch 10/50\n",
      "92/92 [==============================] - 52s 562ms/step - loss: 0.6852 - dice_coef: 0.3147 - val_loss: 0.2443 - val_dice_coef: 0.3200\n",
      "Epoch 11/50\n",
      "92/92 [==============================] - 52s 565ms/step - loss: 0.6639 - dice_coef: 0.3361 - val_loss: 0.3438 - val_dice_coef: 0.3092\n",
      "Epoch 12/50\n",
      "92/92 [==============================] - 52s 569ms/step - loss: 0.6143 - dice_coef: 0.3857 - val_loss: 0.9999 - val_dice_coef: 0.3090\n",
      "Epoch 13/50\n",
      "92/92 [==============================] - 52s 564ms/step - loss: 0.6412 - dice_coef: 0.3588 - val_loss: 0.9999 - val_dice_coef: 0.3109\n",
      "Epoch 14/50\n",
      "92/92 [==============================] - 52s 569ms/step - loss: 0.6337 - dice_coef: 0.3663 - val_loss: 0.3040 - val_dice_coef: 0.3571\n",
      "Epoch 15/50\n",
      "92/92 [==============================] - 52s 565ms/step - loss: 0.6180 - dice_coef: 0.3820 - val_loss: 0.9867 - val_dice_coef: 0.0886\n",
      "Epoch 16/50\n",
      "92/92 [==============================] - 52s 571ms/step - loss: 0.6223 - dice_coef: 0.3777 - val_loss: 0.9999 - val_dice_coef: 0.3760\n",
      "Epoch 17/50\n",
      "92/92 [==============================] - 52s 569ms/step - loss: 0.5886 - dice_coef: 0.4113 - val_loss: 0.7522 - val_dice_coef: 0.1224\n",
      "Epoch 18/50\n",
      "92/92 [==============================] - 52s 566ms/step - loss: 0.6269 - dice_coef: 0.3731 - val_loss: 0.7484 - val_dice_coef: 0.1260\n",
      "Epoch 19/50\n",
      "92/92 [==============================] - 53s 577ms/step - loss: 0.6237 - dice_coef: 0.3762 - val_loss: 0.4646 - val_dice_coef: 0.2473\n",
      "Epoch 20/50\n",
      "92/92 [==============================] - 53s 577ms/step - loss: 0.5964 - dice_coef: 0.4036 - val_loss: 0.6521 - val_dice_coef: 0.1800\n",
      "Epoch 00020: early stopping\n",
      "Time taken:  1060945\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "# This will take ~1.5-2 minutes per epoch on a GPU\n",
    "stmillis = int(round(time.time() * 1000))\n",
    "history = model.fit_generator(generator=training_batch_generator,\n",
    "                    steps_per_epoch=(num_training_samples // batch_size),\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_batch_generator,\n",
    "                    validation_steps=(num_validation_samples // batch_size),\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=5,\n",
    "                    max_queue_size=1,\n",
    "                    callbacks=[checkpointer, early_stopping])\n",
    "endmillis = int(round(time.time() * 1000))\n",
    "print(\"Time taken: \", endmillis - stmillis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xdGCG-kwgF8r"
   },
   "outputs": [],
   "source": [
    "# Save the trained weights\n",
    "model.save(model_name + \".h5\")\n",
    "\n",
    "# Save model config as json\n",
    "model_json = model.to_json()\n",
    "with open(model_name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Persist the model to your google drive [VERY IMPORTANT]\n",
    "!cp unet.* gdrive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Y5XhpA72kAn1",
    "outputId": "d912cf0a-0bcd-478f-8510-03da31529350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 root root    35317 Apr 13 22:53 'gdrive/My Drive/unet.json'\n",
      "-rw------- 1 root root 26259752 Apr 13 22:53 'gdrive/My Drive/unet.h5'\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr gdrive/My\\ Drive/unet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jITgqG2ckFVD"
   },
   "outputs": [],
   "source": [
    "# In case you wish to load your saved model\n",
    "!cp gdrive/My\\ Drive/unet* .\n",
    "\n",
    "model.load_weights(\"unet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2gGNibQmkIpX",
    "outputId": "73bedbee-2289-403f-9de0-f844fe54241e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454,) (454,)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "X_test, y_test =load_data(\"Test\")\n",
    "X_test=np.asarray(X_test)\n",
    "y_test=np.asarray(y_test)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4WV8CbxOkREX"
   },
   "outputs": [],
   "source": [
    "np.random.seed = 629\n",
    "p = np.random.permutation(len(X_test))\n",
    "X_test = X_test[p]\n",
    "y_test = y_test[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3y2jgeujkUly",
    "outputId": "5b196c92-255f-4b9b-fe45-ab42a393bb31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454,) (454,)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "num_epochs = 50\n",
    "print (X_test.shape, y_test.shape)\n",
    "test_data_generator = Surface_Generator(X_test, y_test, batch_size, test=True)\n",
    "y_pred = model.predict_generator(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iGohpk52kcdz",
    "outputId": "0b021ccc-f8ab-4800-982d-712bec35195d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Surface_Generator object at 0x7f6521bb8128>\n",
      "(454, 512, 512, 1)\n",
      "(454, 512, 512, 1)\n",
      "Tensor(\"truediv:0\", shape=(), dtype=float32)\n",
      "-----------\n",
      "Dice coefficient on test data:  0.27833658\n"
     ]
    }
   ],
   "source": [
    "# y_true will have the true masks\n",
    "print(test_data_generator)\n",
    "y_true = test_data_generator.get_all_masks()\n",
    "print(y_true.shape)\n",
    "print(y_pred.shape)\n",
    "print(dice_coef(y_true, y_pred))\n",
    "print(\"-----------\")\n",
    "print (\"Dice coefficient on test data: \", K.get_value(dice_coef(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "7qN3ACRAkixg"
   },
   "outputs": [],
   "source": [
    "# Convert sigmoid outputs to binary class labels\n",
    "y_pred[20][y_pred[20] >= 0.5] = 1\n",
    "y_pred[20][y_pred[20] < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "nLK122gLoqjz",
    "outputId": "10ceb77d-38a4-47ea-d122-ea52e0428997"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(y_true[20][:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "hnMRzIhcotPe",
    "outputId": "789f7aa1-06c7-4588-a329-3e498f21f390"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y_pred[20][:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_b8DRUppovzq",
    "outputId": "137909d0-7d72-4918-f004-41f0b5a76fe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5768451"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.get_value(dice_coef(y_true[20], y_pred[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lGgnrVhwoymP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
